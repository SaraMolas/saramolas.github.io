
**ML Researcher working on mechanistic interpretability, with a background in computational neuroscience and biology.**

I study how complex systems represent information

**Background**
- Biomedical Sciences → Systems & Computational Neuroscience  
- Analyzed high-dimensional neural recordings  
- Studied how spatial and contextual information is encoded by biological neurons

**Current focus**
- Mechanistic interpretability of deep learning models  
- Sparse autoencoders, ablations, training dynamics  
- Understanding how features emerge and interact in artificial neural networks

**Long-term direction**
- Bridge biological and artificial systems
- Make large-scale models more transparent  
- Apply interpretability to foundation models in biology

[Explore my research](research) · [My story(CV)](assets/pdf/CV_MolasMedina.pdf) · [Reach out!](contact) 
