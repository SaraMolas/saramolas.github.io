
Understanding how structured representations emerge in biological and artificial systems.

**Background**
- Biomedical Sciences → Systems & Computational Neuroscience  
- Analyzed high-dimensional neural data
- Studied how spatial and contextual information is encoded by biological neurons

**Current focus**
- Mechanistic interpretability of deep learning models  
- Sparse autoencoders, ablation experiments, training dynamics  
- Understanding feature representation in artificial neural networks

**Long-term direction**
- Bridge biological and artificial systems
- Make large-scale models more transparent  
- Apply interpretability to foundation models in biology

Currently based in London, UK. 
Email: <saramolas18@gmail.com> 

[Explore my research](research) · [My story(CV)](assets/CV_MolasMedina.pdf) · [Find me](contact) 
